{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import dbscan\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_clustering(vectors):\n",
    "    umap_model = umap.UMAP(n_neighbors=75,\n",
    "                           n_components=5,\n",
    "                           metric='cosine').fit(vectors) #model.docvecs.vectors_docs\n",
    "    return umap_model\n",
    "\n",
    "def num_clusters(vectors,mcs):\n",
    "    dm = cosine_distances(vectors)\n",
    "    cluster_ = hdbscan.HDBSCAN(min_cluster_size=mcs,\n",
    "                                  metric='precomputed',\n",
    "                                  cluster_selection_method='eom').fit(dm.astype(np.float64))\n",
    "    cluster_ = len(set(cluster_.labels_)) - 1\n",
    "    \n",
    "    if cluster_ < 3:\n",
    "        cluster_ = 3\n",
    "    return cluster_\n",
    "\n",
    "def gmm_clustering(doc2vec_model, umap_model,n_clusters):\n",
    "    gmm = GaussianMixture(n_components=n_clusters)\n",
    "    gmm.fit(umap_model.embedding_)\n",
    "    cluster_gmm = gmm.predict_proba(umap_model.embedding_)\n",
    "    return cluster_gmm\n",
    "\n",
    "def get_topic_vectors(doc2vec_model, gmm_model, num_clusters):\n",
    "    topic_vectors = np.vstack([doc2vec_model.docvecs.vectors_docs[[c for c,i in enumerate(gmm_model) if np.argmax(i) == x]].mean(axis=0)\n",
    "                               for x in range(num_clusters)])\n",
    "    return topic_vectors\n",
    "\n",
    "def get_topic_words(doc2vec_model,topic_vectors_):\n",
    "    topic_words_ = []\n",
    "    topic_word_scores = []\n",
    "\n",
    "    for tv in topic_vectors_:\n",
    "        sim_words = doc2vec_model.wv.most_similar(positive=[tv], topn=50)\n",
    "        topic_words_.append([word[0] for word in sim_words])\n",
    "        topic_word_scores.append([round(word[1], 4) for word in sim_words])\n",
    "\n",
    "    topic_words_ = np.array(topic_words_)\n",
    "    topic_word_scores = np.array(topic_word_scores)\n",
    "    return topic_words_,topic_word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster(photo):\n",
    "    print(photo)\n",
    "    model = Doc2Vec.load(f'/media/ruben/FEF44259F44213F5/Users/Ruben/Documents/GitHub/iconic-images/analysis/topic-modelling/top2vec/models/doc2vec-{photo}-e75.model')\n",
    "    model.init_sims(replace=False)\n",
    "    umap_model = umap_clustering(model.docvecs.vectors_docs)\n",
    "    mcs = round(len(model.docvecs.vectors_docs) / 100)\n",
    "    if mcs < 15:\n",
    "        mcs = 15\n",
    "    n_cl = num_clusters(umap_model.embedding_,mcs)\n",
    "    print(photo,f\"minimal no. documents: {mcs}, number clusters: {n_cl}\")\n",
    "    gmm_model = gmm_clustering(model,umap_model,n_cl)\n",
    "    topic_vectors_ = get_topic_vectors(model,gmm_model,n_cl)\n",
    "    topic_words,topic_word_scores = get_topic_words(model,topic_vectors_)\n",
    "    data = [[c,\" \".join(t[:100]),sum([x[c] for x in gmm_model])] for c,t in enumerate(topic_words)]\n",
    "    data = pd.DataFrame(data,columns=['cluster','words','prominence'])\n",
    "    data = data.sort_values('prominence',ascending=False)\n",
    "#     for c,i in enumerate(data['words']): \n",
    "#         print(data['cluster'][c],i)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_photos =[x.split('-')[1] for x in os.listdir('/media/ruben/FEF44259F44213F5/Users/Ruben/Documents/GitHub/iconic-images/analysis/topic-modelling/top2vec/models') if \"data\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbuGhraib\n",
      "AbuGhraib minimal no. documents: 15, number clusters: 9\n",
      "AlanKurdi\n",
      "AlanKurdi minimal no. documents: 46, number clusters: 20\n",
      "Anasuma\n",
      "Anasuma minimal no. documents: 15, number clusters: 12\n",
      "Berlin\n",
      "Berlin minimal no. documents: 124, number clusters: 17\n",
      "Camp\n",
      "Camp minimal no. documents: 40, number clusters: 13\n",
      "ChildVulture\n",
      "ChildVulture minimal no. documents: 33, number clusters: 15\n",
      "FallingMan\n",
      "FallingMan minimal no. documents: 38, number clusters: 14\n",
      "Ghandi\n",
      "Ghandi minimal no. documents: 36, number clusters: 17\n",
      "IwoJima\n",
      "IwoJima minimal no. documents: 159, number clusters: 13\n",
      "KentState\n",
      "KentState minimal no. documents: 34, number clusters: 13\n",
      "ManMoon\n",
      "ManMoon minimal no. documents: 138, number clusters: 4\n",
      "Mao\n",
      "Mao minimal no. documents: 15, number clusters: 5\n",
      "Monk\n",
      "Monk minimal no. documents: 50, number clusters: 14\n",
      "NapalmGirl\n",
      "NapalmGirl minimal no. documents: 98, number clusters: 15\n",
      "Plane911\n",
      "Plane911 minimal no. documents: 39, number clusters: 3\n",
      "Hindenburg\n",
      "Hindenburg minimal no. documents: 143, number clusters: 6\n"
     ]
    }
   ],
   "source": [
    "for photo in list_photos:\n",
    "    data = Cluster(photo)\n",
    "    data.to_csv(f'/media/ruben/FEF44259F44213F5/Users/Ruben/Documents/GitHub/iconic-images/analysis/topic-modelling/top2vec/models/results-{photo}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
