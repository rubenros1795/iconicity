{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random, re, string, langid\n",
    "import pandas as pd\n",
    "from func_cluster import *\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import dbscan\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "def umap_clustering(vectors):\n",
    "    umap_model = umap.UMAP(n_neighbors=75,\n",
    "                           n_components=5,\n",
    "                           metric='cosine').fit(vectors) #model.docvecs.vectors_docs\n",
    "    return umap_model\n",
    "\n",
    "def gmm_clustering(umap_emb,n_clusters):\n",
    "    gmm = GaussianMixture(n_components=n_clusters,random_state=17)\n",
    "    gmm.fit(umap_emb)\n",
    "    cluster_gmm = gmm.predict_proba(umap_emb)\n",
    "    return cluster_gmm\n",
    "\n",
    "def get_topic_vectors(doc2vec_model, gmm_model, num_clusters):\n",
    "    topic_vectors = np.vstack([doc2vec_model.docvecs.vectors_docs[[c for c,i in enumerate(gmm_model) if np.argmax(i) == x]].mean(axis=0)\n",
    "                               for x in range(num_clusters)])\n",
    "    return topic_vectors\n",
    "\n",
    "def get_topic_words(doc2vec_model,topic_vectors_):\n",
    "    topic_words_ = []\n",
    "    topic_word_scores = []\n",
    "\n",
    "    for tv in topic_vectors_:\n",
    "        sim_words = doc2vec_model.wv.most_similar(positive=[tv], topn=50)\n",
    "        topic_words_.append([word[0] for word in sim_words])\n",
    "        topic_word_scores.append([round(word[1], 4) for word in sim_words])\n",
    "\n",
    "    topic_words_ = np.array(topic_words_)\n",
    "    topic_word_scores = np.array(topic_word_scores)\n",
    "    return topic_words_,topic_word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = [os.path.split(x)[-1].split('-')[1] for x in glob.glob('C:/Users/ruben.ros/Documents/REACT/data/embeddings/doc2vec/*.model')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for photo in photos:\n",
    "#     model = Doc2Vec.load(f'D:/react-data/iconic/models/doc2vec-{photo}-e75.model')\n",
    "#     model.init_sims(replace=False)\n",
    "#     umap_model = umap_clustering(model.docvecs.vectors_docs)\n",
    "#     np.savetxt(f'C:/Users/ruben.ros/Documents/REACT/data/doc2vec/models/{photo}-umap-embeddings.csv', umap_model.embedding_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 clusters in  AbuGhraib\n",
      "12 clusters in  AlanKurdi\n",
      "8 clusters in  Anasuma\n",
      "19 clusters in  Berlin\n",
      "13 clusters in  Camp\n",
      "17 clusters in  Che\n",
      "16 clusters in  ChildVulture\n",
      "4 clusters in  ChileCoup\n",
      "13 clusters in  FallingMan\n",
      "13 clusters in  Ghandi\n",
      "19 clusters in  Hindenburg\n",
      "10 clusters in  IwoJima\n",
      "14 clusters in  KentState\n",
      "16 clusters in  ManMoon\n",
      "3 clusters in  Mao\n",
      "23 clusters in  MigrantMother\n",
      "19 clusters in  Monk\n",
      "15 clusters in  NapalmGirl\n",
      "13 clusters in  Plane911\n",
      "4 clusters in  Rwanda\n",
      "10 clusters in  SharbatGula\n",
      "9 clusters in  SpanishSoldier\n",
      "14 clusters in  TankMan\n",
      "15 clusters in  TimesSquareKiss\n",
      "8 clusters in  VietCong\n",
      "7 clusters in  WarRoom\n"
     ]
    }
   ],
   "source": [
    "for photo in photos:\n",
    "    ## Determine number of clusters\n",
    "    num_clus = pd.read_csv(f'C:/Users/ruben.ros/Documents/REACT/data/evaluation/results-{photo}-bicaic-eval.csv')\n",
    "    num_clus = len(num_clus['cluster'])\n",
    "    print(num_clus, \"clusters in \", photo)\n",
    "    \n",
    "    ## Load UMAP embeddings\n",
    "    umap_emb = np.loadtxt(f'C:/Users/ruben.ros/Documents/REACT/data/embeddings/umap/{photo}-umap-embeddings.csv', delimiter=',')\n",
    "    \n",
    "    ## GMM Clustering\n",
    "    gmm_model = gmm_clustering(umap_emb,num_clus)\n",
    "    \n",
    "    ## Load Doc2Vec\n",
    "    model = Doc2Vec.load(f'C:/Users/ruben.ros/Documents/REACT/data/embeddings/doc2vec/doc2vec-{photo}-e75.model')\n",
    "    model.init_sims(replace=False)\n",
    "    \n",
    "    ## Get Topic Vectors and Top Words\n",
    "    topic_vectors_ = get_topic_vectors(model,gmm_model,num_clus)\n",
    "    topic_words,topic_word_scores = get_topic_words(model,topic_vectors_)\n",
    "    \n",
    "    ## Export Cluster Distributions\n",
    "    metadata = pd.read_csv(f'C:/Users/ruben.ros/Documents/REACT/data/embeddings/metadata/{photo}-metadata.csv')\n",
    "    metadata = {row[0]:[row[1],row[2],row[3]] for ind,row in metadata.iterrows()}\n",
    "    \n",
    "    d = []\n",
    "    for index_,x in enumerate(gmm_model):\n",
    "        if int(index_) in list(metadata.keys()) and metadata[int(index_)][1] != \"nan\":\n",
    "            id_ = metadata[int(index_)][0]\n",
    "            year = metadata[int(index_)][1]\n",
    "            d.append([id_] + [year] + x.tolist())\n",
    "        else:\n",
    "            id_year = \"nan\"\n",
    "            id_ = \"nan\"\n",
    "            year = \"nan\"\n",
    "            d.append([id_] + [year] + x.tolist())\n",
    "    data = pd.DataFrame(d,columns=[\"id\",\"year\"] + [\"c\" + str(x) for x in range(0,num_clus)])\n",
    "    data = data[data['id'] != \"nan\"]\n",
    "    data = data[data['year'] != \"nan\"]\n",
    "    data = data.reset_index(drop=True)\n",
    "    data['year'] = data['year'].astype(int)\n",
    "    topwords = pd.DataFrame([\" \".join(w) for w in topic_words],columns=['topwords'])\n",
    "    topwords = {\"c\"+str(c):\" \".join(i.split(' ')[:num_clus]) for c,i in enumerate(topwords['topwords'])}\n",
    "    data.columns = [\"id\",\"year\"] + [topwords[c] for c in list(data.columns)[2:]]\n",
    "\n",
    "    d = pd.DataFrame()\n",
    "\n",
    "    for year in range(1995,2020):\n",
    "        ss = data[data['year'] == year]\n",
    "        if len(ss) == 0:\n",
    "            continue\n",
    "        sum_d = []\n",
    "        for i in range(0,num_clus):\n",
    "            sum_ = ss[topwords['c' + str(i)]].sum()\n",
    "            sum_d.append(sum_)\n",
    "        d[str(year)] = sum_d\n",
    "        \n",
    "    for year in d.columns:\n",
    "        d[year] = [i / d[year].sum() for i in d[year]]\n",
    "        \n",
    "    d = d.T\n",
    "    d = d.reset_index()\n",
    "    d.columns = ['year'] + [topwords['c' + str(c)] for c in list(d.columns)[1:]]\n",
    "    d.to_csv(f'C:/Users/ruben.ros/Documents/REACT/data/results/diachronic-plots/{photo}-data.csv',index=False)\n",
    "    \n",
    "    ## Generate Top Words\n",
    "    twdf = pd.DataFrame(topic_words).T\n",
    "    twdf.columns = [f\"cluster_{x}\" for x in list(range(num_clus + 1))[1:]]\n",
    "    twdf.to_csv(f'C:/Users/ruben.ros/Documents/REACT/data/results/top-words/{photo}-data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
